{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LCEL Chain ë©”ëª¨ë¦¬ ì¶”ê°€**\n",
    "\n",
    "ëŒ€í™” ë‚´ìš© ê¸°ì–µ\n",
    "\n",
    "ì„ì˜ì˜ ì²´ì¸ì— ë©”ëª¨ë¦¬ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_groq import ChatGroq\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ëª¨ë¸ ì •ì˜\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [   \n",
    "        ('system', 'You are a helpful AI assitants'),\n",
    "        MessagesPlaceholder(variable_name='chat_history'),\n",
    "        ('user', '{input}')\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat_history': []}\n"
     ]
    }
   ],
   "source": [
    "# ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ ìƒì„±, ë©”ì„¸ì§€ ë°˜í™˜ ê¸°ëŠ¥ í™œì„±í™”\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key = 'chat_history')  # ëŒ€í™” ê¸°ì–µì„ ì €ì¥í•˜ê¸° ìœ„í•œ ë©”ëª¨ë¦¬ë¦¬\n",
    "print(memory.load_memory_variables({})) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnablePassthrough.assign` ì‚¬ìš©\n",
    "\n",
    "`chat_history` ë³€ìˆ˜ì— `memory.load_memory_variables` í•¨ìˆ˜ì˜ ê²°ê³¼ë¥¼ í• ë‹¹í•˜ê³ , ì´ ê²°ê³¼ì—ì„œ `chat_history` í‚¤ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì¶”ì¶œ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "runnable = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables)  # memory.load_memory_variablesë¥¼ ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•œ RunnableLambda\n",
    "    | itemgetter(\"chat_history\")  # ë”•ì…”ë„ˆë¦¬ì˜ í‚¤ê°’ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ itemgetter(chat_historyê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê¸° ë•Œë¬¸ì—ì—)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'ì•ˆë…•í•˜ì„¸ìš”', 'chat_history': {'chat_history': []}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itemgeter ì•ˆì“¸ë•Œ \n",
    "runnable.invoke({'input' : 'ì•ˆë…•í•˜ì„¸ìš”'})   # runnable.assignì€ ë°˜ë“œì‹œ ë”•ì…”ë„ˆë¦¬ë¡œ ì…ë ¥í•´ì•¼í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'ì•ˆë…•í•˜ì„¸ìš”', 'chat_history': []}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# itemgeter ì“¸ë•Œ\n",
    "runnable.invoke({'input' : 'ì•ˆë…•í•˜ì„¸ìš”'})  # 'chat_history' : []  => ë”•ì…”ë„ˆë¦¬ì—ì„œ í‚¤ê°’ì„ ê°€ì ¸ì™€ ë¦¬ìŠ¤íŠ¸ë§Œ ì¶œë ¥ë¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”! ì§ì¥ê³¼ ëŒ€í•™ì›ê¹Œì§€ ë³‘í–‰í•˜ì‹œëŠ”êµ°ìš”. ì •ë§ ë©‹ì§€ê³  í˜ë“  ì¼ì´ê² ë„¤ìš”.  \n",
      "\n",
      "ì–´ë–¤ ë¶„ì•¼ë¥¼ ê³µë¶€í•˜ê³  ê³„ì‹ ê°€ìš”? í˜¹ì‹œ í˜ë“  ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”?  \n",
      "\n",
      "ì¡°ê¸ˆì´ë¼ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ì–´ìš”. ğŸ˜Š ğŸ’ª\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain ìƒì„±\n",
    "chain = runnable | prompt | llm\n",
    "\n",
    "# ì‘ë‹µ ìƒì„±\n",
    "response = chain.invoke({'input' : \"ì•ˆë…•í•˜ì„¸ìš” ì§ì¥ì´ê³  ëŒ€í•™ì›ì„ ë‹¤ë‹ˆê³  ìˆì–´ìš”\"})\n",
    "print(response.content)  # ìƒì„±ëœ ì‘ë‹µì„ ì¶œë ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': [HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹  \\n\\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š  \\n\\nì €ëŠ” ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ë‹µí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ë²ˆì—­ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \\n\\nê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š\\n', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì•„ì§ ì €ì¥ ì•ˆë¨\n",
    "memory.load_memory_variables({}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì…ë ¥ ë°ì´í„°ì™€ ì‘ë‹µì„ ë©”ëª¨ë¦¬ì— ì €ì¥ \n",
    "memory.save_context(\n",
    "    {\"user\": \"ì•ˆë…•í•˜ì„¸ìš” ì§ì¥ì´ê³  ëŒ€í•™ì›ì„ ë‹¤ë‹ˆê³  ìˆì–´ìš”\"}, {\"ai\": response.content}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ğŸ‘‹  \\n\\në¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? ğŸ˜Š  \\n\\nì €ëŠ” ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ë‹µí•˜ê³ , í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ë²ˆì—­ì„ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \\n\\nê¶ê¸ˆí•œ ê²ƒì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”! ğŸ˜Š\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš” ì§ì¥ì´ê³  ëŒ€í•™ì›ì„ ë‹¤ë‹ˆê³  ìˆì–´ìš”', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”! ì§ì¥ê³¼ ëŒ€í•™ì›ê¹Œì§€ ë³‘í–‰í•˜ì‹œëŠ”êµ°ìš”. ì •ë§ ë©‹ì§€ê³  í˜ë“  ì¼ì´ê² ë„¤ìš”.  \\n\\nì–´ë–¤ ë¶„ì•¼ë¥¼ ê³µë¶€í•˜ê³  ê³„ì‹ ê°€ìš”? í˜¹ì‹œ í˜ë“  ì¼ì´ ìˆìœ¼ì‹ ê°€ìš”?  \\n\\nì¡°ê¸ˆì´ë¼ë„ ë„ì›€ì´ ë  ìˆ˜ ìˆìœ¼ë©´ ì¢‹ê² ì–´ìš”. ğŸ˜Š ğŸ’ª\\n', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë©”ëª¨ë¦¬ ì €ì¥ê¸°ë¡ ì¶œë ¥\n",
    "memory.load_memory_variables({})['chat_history']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë„¤, ê¸°ì–µí•´ìš”! ì§ì¥ê³¼ ëŒ€í•™ì›ì„ ë³‘í–‰í•˜ê³  ê³„ì‹ ë‹¤ê³  ë§ì”€í•˜ì…¨ì–´ìš”.  ì •ë§ ë©‹ì§€ë„¤ìš”!  ì–´ë–¤ ë¶„ì•¼ë¥¼ ê³µë¶€í•˜ê³  ê³„ì‹ ê°€ìš”? ğŸ˜Š\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ê¸°ì–µí•˜ê³  ìˆëŠ”ì§€ í™•ì¸\n",
    "response = chain.invoke({\"input\": \"ì œê°€ ì–´ë–¤ê±¸ í•˜ê³  ìˆë‹¤ê³  ë§í–ˆëŠ”ì§€ ê¸°ì–µí•´?\"})\n",
    "# ë‹µë³€ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ì»¤ìŠ¤í…€ ConversationChain êµ¬í˜„ ì˜ˆì‹œ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, Runnable\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# ChatOpenAI ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# ëŒ€í™”í˜• í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì‹œìŠ¤í…œ ë©”ì‹œì§€, ì´ì „ ëŒ€í™” ë‚´ì—­, ê·¸ë¦¬ê³  ì‚¬ìš©ì ì…ë ¥ì„ í¬í•¨í•©ë‹ˆë‹¤.\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful chatbot\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬ë¥¼ ìƒì„±í•˜ê³ , ë©”ì‹œì§€ ë°˜í™˜ ê¸°ëŠ¥ì„ í™œì„±í™”í•©ë‹ˆë‹¤.\n",
    "memory = ConversationBufferMemory(return_messages=True, memory_key=\"chat_history\")\n",
    "\n",
    "class MyConversationChain(Runnable):\n",
    "\n",
    "    def __init__(self, llm, prompt, memory, input_key=\"input\"):\n",
    "\n",
    "        self.prompt = prompt\n",
    "        self.memory = memory\n",
    "        self.input_key = input_key\n",
    "\n",
    "        self.chain = (\n",
    "            RunnablePassthrough.assign(\n",
    "                chat_history=RunnableLambda(self.memory.load_memory_variables)\n",
    "                | itemgetter(memory.memory_key)  # memory_key ì™€ ë™ì¼í•˜ê²Œ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "            )\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "\n",
    "    def invoke(self, query, configs=None, **kwargs):\n",
    "        answer = self.chain.invoke({self.input_key: query})\n",
    "        self.memory.save_context(inputs={\"human\": query}, outputs={\"ai\": answer})\n",
    "        return answer\n",
    "\n",
    "conversation_chain = MyConversationChain(llm, prompt, memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”, í…Œë””ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì•ˆë…•í•˜ì„¸ìš”? ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í…Œë”” ì…ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë‹¹ì‹ ì˜ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤! ë§ë‚˜ìš”?'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì œ ì´ë¦„ì´ ë­ë¼ê³ ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Got it! I'll respond in English from now on. How can I assist you today?\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì•ìœ¼ë¡œëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš” ì•Œê² ì–´ìš”?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your name is Teddy.'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.invoke(\"ì œ ì´ë¦„ì„ ë‹¤ì‹œ í•œ ë²ˆ ë§í•´ì£¼ì„¸ìš”\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='ì•ˆë…•í•˜ì„¸ìš”? ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì œ ì´ë¦„ì€ í…Œë”” ì…ë‹ˆë‹¤.', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ì•ˆë…•í•˜ì„¸ìš”, í…Œë””ë‹˜! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ì–´ë–»ê²Œ ë„ì™€ë“œë¦´ê¹Œìš”?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì œ ì´ë¦„ì´ ë­ë¼ê³ ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='ë‹¹ì‹ ì˜ ì´ë¦„ì€ í…Œë””ì…ë‹ˆë‹¤! ë§ë‚˜ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì•ìœ¼ë¡œëŠ” ì˜ì–´ë¡œë§Œ ë‹µë³€í•´ì£¼ì„¸ìš” ì•Œê² ì–´ìš”?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Got it! I'll respond in English from now on. How can I assist you today?\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='ì œ ì´ë¦„ì„ ë‹¤ì‹œ í•œ ë²ˆ ë§í•´ì£¼ì„¸ìš”', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Your name is Teddy.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_chain.memory.load_memory_variables({})[\"chat_history\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
