{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b099f91d",
   "metadata": {},
   "source": [
    "# Arxiv\n",
    "\n",
    "[arXiv](https://arxiv.org/)에서 특정 주제를 검색하여 불러옴\n",
    "\n",
    "\n",
    "[API 도큐먼트](https://api.python.langchain.com/en/latest/document_loaders/langchain_community.document_loaders.arxiv.ArxivLoader.html#langchain_community.document_loaders.arxiv.ArxivLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2694ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설치\n",
    "# !pip install -qU langchain-community arxiv pymupdf\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "# 논문 주제 입력력\n",
    "loader = ArxivLoader(\n",
    "    query=\"Attention\",  \n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=True,  # 메타데이터 전체 로드 여부\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3037bd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f18bc2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Published': '2023-06-02',\n",
       " 'Title': 'RITA: Group Attention is All You Need for Timeseries Analytics',\n",
       " 'Authors': 'Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li',\n",
       " 'Summary': \"Timeseries analytics is of great importance in many real-world applications.\\nRecently, the Transformer model, popular in natural language processing, has\\nbeen leveraged to learn high quality feature embeddings from timeseries, core\\nto the performance of various timeseries analytics tasks. However, the\\nquadratic time and space complexities limit Transformers' scalability,\\nespecially for long timeseries. To address these issues, we develop a\\ntimeseries analytics tool, RITA, which uses a novel attention mechanism, named\\ngroup attention, to address this scalability issue. Group attention dynamically\\nclusters the objects based on their similarity into a small number of groups\\nand approximately computes the attention at the coarse group granularity. It\\nthus significantly reduces the time and space complexity, yet provides a\\ntheoretical guarantee on the quality of the computed attention. The dynamic\\nscheduler of RITA continuously adapts the number of groups and the batch size\\nin the training process, ensuring group attention always uses the fewest groups\\nneeded to meet the approximation quality requirement. Extensive experiments on\\nvarious timeseries datasets and analytics tasks demonstrate that RITA\\noutperforms the state-of-the-art in accuracy and is significantly faster --\\nwith speedups of up to 63X.\",\n",
       " 'entry_id': 'http://arxiv.org/abs/2306.01926v1',\n",
       " 'published_first_time': '2023-06-02',\n",
       " 'comment': None,\n",
       " 'journal_ref': None,\n",
       " 'doi': None,\n",
       " 'primary_category': 'cs.LG',\n",
       " 'categories': ['cs.LG', 'cs.AI', 'cs.DB'],\n",
       " 'links': ['http://arxiv.org/abs/2306.01926v1',\n",
       "  'http://arxiv.org/pdf/2306.01926v1']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서 메타데이터 출력\n",
    "docs[2].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58a907",
   "metadata": {},
   "source": [
    "`load_all_available_meta=False` 인 경우 메타데이터는 전체가 아닌 일부만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb84ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query 에 검색하고자 하는 논문의 주제를 입력합니다.\n",
    "loader = ArxivLoader(\n",
    "    query=\"ChatGPT\",\n",
    "    load_max_docs=2,  # 최대 문서 수\n",
    "    load_all_available_meta=False,  # 메타데이터 전체 로드 여부\n",
    ")\n",
    "\n",
    "# 문서 로드 결과출력\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서 메타데이터 출력\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70249aec",
   "metadata": {},
   "source": [
    "### **요약(summary)**\n",
    "\n",
    "- `get_summaries_as_docs()` => 논문의 요약본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8117a593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inference demand for LLMs has skyrocketed in recent months, and serving\n",
      "models with low latencies remains challenging due to the quadratic input length\n",
      "complexity of the attention layers. In this work, we investigate the effect of\n",
      "dropping MLP and attention layers at inference time on the performance of\n",
      "Llama-v2 models. We find that dropping dreeper attention layers only marginally\n",
      "decreases performance but leads to the best speedups alongside dropping entire\n",
      "layers. For example, removing 33\\% of attention layers in a 13B Llama2 model\n",
      "results in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\n",
      "also observe that skipping layers except the latter layers reduces performances\n",
      "for more layers skipped, except for skipping the attention layers.\n"
     ]
    }
   ],
   "source": [
    "# 문서 요약 로딩\n",
    "docs = loader.get_summaries_as_docs()\n",
    "\n",
    "# 첫 번째 문서 접근\n",
    "print(docs[0].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
