{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95c6eb01",
   "metadata": {},
   "source": [
    "### **CacheBackedEmbeddings**  \n",
    "\n",
    "`CacheBackedEmbeddings`는 임베딩을 캐싱하여 불필요한 **재계산을 방지**하는 도구\n",
    "\n",
    "키-값 저장소를 활용하여 동일한 텍스트의 임베딩을 다시 계산하지 않고, 캐시(임시저장소)에서 바로 불러올 수 있도록 함.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **🧐 왜 필요할까?**  \n",
    "\n",
    "✔ **성능 최적화** – 임베딩을 캐싱하여 불필요한 계산을 줄이고 속도를 향상함.  \n",
    "✔ **비용 절감** – API 기반 임베딩 서비스 사용 시 반복 계산을 줄여 비용을 절감할 수 있음.  \n",
    "✔ **일관성 유지** – 동일한 텍스트에 대해 항상 동일한 임베딩 값을 제공함.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **🔍 어떻게 작동할까?**  \n",
    "\n",
    "1️⃣ **텍스트를 해시(hash)하여 고유 키 생성**  \n",
    "2️⃣ **캐시에 해당 키가 있으면 저장된 임베딩 반환**  \n",
    "3️⃣ **없으면 `underlying_embeddings`를 사용하여 계산 후 캐시에 저장**  \n",
    "\n",
    "📌 **초기화 방법: `from_bytes_store`**  \n",
    "\n",
    "- `underlying_embeddings`: 임베딩을 수행하는 embedder 모델  \n",
    "- `document_embedding_cache`: 문서 임베딩을 저장하는 `ByteStore`  \n",
    "- `namespace` (선택 사항): 임베딩 캐시의 충돌 방지를 위해 사용됨  \n",
    "\n",
    "---\n",
    "\n",
    "#### **🛠 어디에 활용할 수 있을까?**  \n",
    "\n",
    "✅ **대량의 문서 분석** – 동일한 텍스트의 반복 분석 시 캐싱을 활용하여 속도를 높임.  \n",
    "✅ **AI 및 NLP 시스템** – AI 모델이 동일한 입력을 반복적으로 처리할 때 성능 향상.  \n",
    "✅ **검색 및 추천 시스템** – 임베딩 기반 검색에서 연산 비용 절감.  \n",
    "\n",
    "---\n",
    "\n",
    "#### **🎯 한 줄 요약**  \n",
    "\n",
    "**CacheBackedEmbeddings**는 임베딩을 캐싱하여 **성능을 최적화하고 불필요한 재계산을 방지하는 도구**임. 빠르고 효율적인 NLP 및 AI 시스템 구축에 도움을 줌. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf94c21a",
   "metadata": {},
   "source": [
    "### **LocalFileStore 에서 임베딩 사용 (영구 보관)**\n",
    "\n",
    "- 로컬 파일 시스템을 사용하여 임베딩을 저장(파일로 만들어서 보관)\n",
    "- FAISS 벡터 스토어를 사용하여 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6d2d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores.faiss import FAISS\n",
    "\n",
    "# OpenAI 임베딩\n",
    "embedding = OpenAIEmbeddings(model = 'text-embedding-3-small')\n",
    "\n",
    "# 로컬 파일 저장소 설정\n",
    "store = LocalFileStore(\"./cache/\")\n",
    "\n",
    "# 캐시를 지원하는 임베딩 생성\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings = embedding,\n",
    "    document_embedding_cache = store,\n",
    "    namespace = embedding.model\n",
    "    \n",
    ")\n",
    "\n",
    "# store에서 키 조회\n",
    "list(store.yield_keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a873ccce",
   "metadata": {},
   "source": [
    "1. 문서로드\n",
    "2. 청크 분할\n",
    "3. 청크 임베딩\n",
    "4. 벡터 저장소에 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e6873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 문서 로드\n",
    "raw_documents = TextLoader(\"./data/appendix-keywords.txt\").load()\n",
    "\n",
    "# 문자 단위로 텍스트 분할 설정\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "\n",
    "# 문서 분할\n",
    "documents = text_splitter.split_documents(raw_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e31b91",
   "metadata": {},
   "source": [
    "%%time \n",
    "\n",
    "셀 전체 코드의 실행시간 상단에 쓰면됨 \n",
    "\n",
    "%time \n",
    "한줄 사용 변수 앞에 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442c979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# 코드 실행 시간을 측정합니다.\n",
    "%time db = FAISS.from_documents(documents, cached_embedder)  # 문서로부터 FAISS 데이터베이스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487b6402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "# 캐싱된 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "%%time\n",
    "db2 = FAISS.from_documents(documents, cached_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf59fa6",
   "metadata": {},
   "source": [
    "### **`InmemoryByteStore` 사용 (비영구적)**\n",
    "\n",
    "다른 `ByteStore`를 사용하기 위해서는 `CacheBackedEmbeddings`를 생성할 때 해당 `ByteStore`를 사용.\n",
    "\n",
    "메모리에 일시적으로 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a58c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain.storage import InMemoryByteStore\n",
    "\n",
    "store = InMemoryByteStore()  # 메모리 내 바이트 저장소 생성\n",
    "\n",
    "# 캐시 지원 임베딩 생성\n",
    "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
    "    embedding, store, namespace=embedding.model\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
