{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model**\n",
    "\n",
    "- Openai : OPENAI 유료 API\n",
    "- Ollama : 무료 무제한 사용 PC 성능에 따라 제한 Local 모델\n",
    "- Groq : API 정해진 횟수 만큼 사용가능 ex) Llama 70B, 405B 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- langchain: LLM 기반 체인과 에이전트를 구축하는 핵심 라이브러리.\n",
    "\n",
    "\n",
    "- langchain-core: LangChain의 기본 추상화와 표현 언어를 제공.\n",
    "\n",
    "\n",
    "- langchain-experimental: 실험적인 기능과 도구를 포함.\n",
    "\n",
    "\n",
    "- langchain-community: 커뮤니티 기여로 유지되는 서드파티 통합을 제공.\n",
    "\n",
    "\n",
    "- langchain-openai: OpenAI 모델을 LangChain에 연결.\n",
    "\n",
    "\n",
    "- langchain-teddynote: 테디노트 LangChain\n",
    "\n",
    "\n",
    "- langchain-huggingface: Hugging Face 모델을 통합.\n",
    "\n",
    "\n",
    "- langchain-google-genai: Google 생성형 AI를 활용.\n",
    "\n",
    "\n",
    "- langchain-anthropic: Anthropic 모델을 연결.\n",
    "\n",
    "\n",
    "- langchain-cohere: Reranker.\n",
    "\n",
    "\n",
    "- langchain-chroma: Chroma 벡터 데이터베이스와 통합.\n",
    "\n",
    "\n",
    "- langchain-elasticsearch: Elasticsearch로 검색 기능을 강화.\n",
    "\n",
    "\n",
    "- langchain-upstage: Upstage 모델 연결.\n",
    "\n",
    "\n",
    "- langchain-milvus: Milvus 벡터 데이터베이스를 지원.\n",
    "\n",
    "\n",
    "- langchain-text-splitters: 텍스트를 조각으로 나누는 유틸리티를 제공."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain-openai\n",
    "!pip install langchain_community\n",
    "!pip install langchain_ollama\n",
    "!pip install langchain-groq\n",
    "!pip install langchain-experimental\n",
    "!pip install langchain-cohere\n",
    "!pip install langchain-elasticsearch\n",
    "!pip install langchain-milvus\n",
    "!pip install langchain-teddynote\n",
    "\n",
    "# openai tokenzier\n",
    "!pip install tiktoken  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getenv(\"GROQ_API_KEY\"))\n",
    "print(os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "\n",
    "response = llm.invoke('안녕 너의 소개를 해줄래?')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Ollama**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama 둘다 사용가능\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# model \n",
    "llm = ChatOllama(model = 'gemma')\n",
    "\n",
    "answer = llm.invoke('지구의 자전 주기에 대해 알려주세요')\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Groq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "response = llm.invoke('안녕 너의 소개를 해줄래?')\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
