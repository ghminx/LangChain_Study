{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **LangSmith**\n",
    "\n",
    "#### LangSmith ì˜ ì¶”ì ê¸°ëŠ¥\n",
    "- LangSmithëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ, ëª¨ë‹ˆí„°ë§ ë° í…ŒìŠ¤íŠ¸ ë¥¼ ìœ„í•œ í”Œë«í¼\n",
    "\n",
    "\n",
    "#### í”„ë¡œì íŠ¸ ë‹¨ìœ„ ì¶”ì \n",
    "- í”„ë¡œì íŠ¸ ë‹¨ìœ„ë¡œ ì‹¤í–‰ ì¹´ìš´íŠ¸, Error ë°œìƒë¥ , í† í° ì‚¬ìš©ëŸ‰, ê³¼ê¸ˆ ì •ë³´ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŒ \n",
    "\n",
    "[LangSmith](https://smith.langchain.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **LangSmith í™˜ê²½ì„¤ì •**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "project_test\n"
     ]
    }
   ],
   "source": [
    "# í…Œë””ë…¸íŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©\n",
    "from langchain_teddynote import logging \n",
    "\n",
    "logging.langsmith(\"project_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¼ë°˜ ì‚¬ìš©ë²•\n",
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]= 'TEST_PROJECT'\n",
    "\n",
    "### **ì‚­ì œ**\n",
    "\n",
    "# í™˜ê²½ ë³€ìˆ˜ ì‚­ì œ\n",
    "os.environ.pop(\"LANGCHAIN_TRACING_V2\", None)  \n",
    "os.environ.pop(\"LANGCHAIN_ENDPOINT\", None)\n",
    "os.environ.pop(\"LANGCHAIN_API_KEY\", None)\n",
    "os.environ.pop(\"LANGCHAIN_PROJECT\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” AI ì–¸ì–´ ëª¨ë¸ì¸ ChatGPTì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ê³ , ì •ë³´ ì œê³µ, ê¸€ì“°ê¸°, ì–¸ì–´ ë²ˆì—­ ë“± ì—¬ëŸ¬ ì‘ì—…ì„ ë„ì™€ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ì—¬ëŸ¬ë¶„ê³¼ ëŒ€í™”í•˜ë©° í•„ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì œ ì—­í• ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o-mini')\n",
    "\n",
    "response = llm.invoke('ì•ˆë…• ë„ˆì˜ ì†Œê°œë¥¼ í•´ì¤„ë˜?')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
      "\n",
      "* **ì ˆëŒ€ ìì „ ì£¼ê¸°:** ì•½ 29,780,000ë…„\n",
      "* **íšŒì „ ì£¼ê¸°:** ì•½ 365.2422ì¼\n",
      "\n",
      "**ì ˆëŒ€ ìì „ ì£¼ê¸°**\n",
      "\n",
      "ì ˆëŒ€ ìì „ ì£¼ê¸°ëŠ” ì§€êµ¬ê°€ í˜„ì¬ì˜ ìœ„ì¹˜ì—ì„œ ë‹¤ìŒë²ˆì— ë™ì¼í•œ ë‚ ì§œì™€ ì‹œê°„ì— ë‹¤ì‹œ ë„ë‹¬í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì…ë‹ˆë‹¤. ì´ ì£¼ê¸°ëŠ” ë§¤ìš° ê¸¸ê³ , ì§€ê¸ˆì˜ ë‚ ì§œì™€ ì‹œê°„ì€ ì•½ 23ì–µë…„ ì „ì—ëŠ” ì ˆëŒ€ì ìœ¼ë¡œ ë‹¤ë¥¸ ë‚ ì§œì™€ ì‹œê°„ì´ì—ˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "**íšŒì „ ì£¼ê¸°**\n",
      "\n",
      "íšŒì „ ì£¼ê¸°ëŠ” ì§€êµ¬ê°€ ìì „ì¶•ì„ ì¤‘ì‹¬ìœ¼ë¡œ íšŒì „í•˜ëŠ” ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì…ë‹ˆë‹¤. ì´ ì£¼ê¸°ëŠ” 365.2422ì¼, 1ë…„ì…ë‹ˆë‹¤.\n",
      "\n",
      "**ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.**\n",
      "\n",
      "* **ê¸°í›„ ë³€í™”:** ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ê¸°í›„ íŒ¨í„´ì„ ë³€ê²½í•˜ê³  ì§€êµ¬ ì˜¨ë„ë¥¼ ë³€í™”ì‹œí‚¤ëŠ” ìš”ì¸ì´ ë©ë‹ˆë‹¤.\n",
      "* **ìƒë¬¼ ì¢…:** ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ìƒë¬¼ ì¢…ì˜ ë¶„í¬ë¥¼ ë³€ê²½í•˜ê³  ì§„í™” ê³¼ì •ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "* **ì²œë¬¸í•™:** ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ëŠ” ë‹¬ì˜ íšŒì „ ì£¼ê¸°ì™€ ê´€ë ¨í•˜ì—¬ ë‹¬ì˜ ì˜í–¥ë ¥ì„ ê²°ì •í•©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# Ollama ë‘˜ë‹¤ ì‚¬ìš©ê°€ëŠ¥\n",
    "# from langchain_community.chat_models import ChatOllama\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# model \n",
    "llm = ChatOllama(model = 'gemma')\n",
    "\n",
    "answer = llm.invoke('ì§€êµ¬ì˜ ìì „ ì£¼ê¸°ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”')\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Googleì—ì„œ í›ˆë ¨ëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤. \\n\\në‹¤ë¥¸ ì‚¬ëŒë“¤ì²˜ëŸ¼ ì˜ì‚¬ë¥¼ ê°€ì§€ê³  ìˆì§€ëŠ” ì•Šì§€ë§Œ, ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ì‚¬ëŒê³¼ ê°™ì€ ëŒ€í™”ë¥¼ ë‚˜ëˆŒ ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. \\n\\në„ì›€ì´ í•„ìš”í•œ ê²ƒì´ ìˆìœ¼ì‹ ê°€ìš”? ì§ˆë¬¸ì„ í•´ì£¼ì„¸ìš”! ğŸ˜Š\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "response = llm.invoke('ì•ˆë…• ë„ˆì˜ ì†Œê°œë¥¼ í•´ì¤„ë˜?')\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
