{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **StructuredOutputParser(구조화된 출력 파서)**\n",
    "\n",
    "- 답변을 `dict` 형식으로 구성, Key/Value 쌍으로 갖는 여러 필드를 반환 할때 사용 \n",
    "- Pydantic/JSON이 더 유용하지만 파라미터가 적은 모델에 적합함(로컬 모델은 Pydantic 파서가 동작하지 않는 경우가 많음)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `ResponseSchema` 사용자의 질문에 대한 답변과 사용된 소스(웹사이트)에 대한 설명을 포함하는 응답 스키마를 정의\n",
    "\n",
    "- `StructuredOutputParser`를 `response_schemas`를 사용하여 초기화하여, 정의된 응답 스키마에 따라 출력을 구조화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# 사용자의 질문에 대한 답변 \n",
    "# 로컬 모델은 한글 사용시 부족할 수 있음 영어로 적는걸 권장\n",
    "response_schemas = [\n",
    "    ResponseSchema(name = 'answer', description = '사용자의 질문에 대한 답변'),\n",
    "    ResponseSchema(name = 'source', description = '사용자의 질문에 답하기 위해 사용된 `출처`, `참고` 등의 정보'),\n",
    "]\n",
    "\n",
    "# 응답 스키마를 기반으로 구조화된 파서 초기화 \n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 형식 지시사항 \n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template = 'answer the users question as best as possible. \\n {format_instructions} \\n {question}',\n",
    "    \n",
    "    # 입력변수 \n",
    "    input_variables=['question'],\n",
    "    \n",
    "    # 부분변수 \n",
    "    partial_variables={'format_instructions' : format_instructions})\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model = 'gemma2-9b-it', temperature=0)  \n",
    "chain = prompt | model | output_parser  \n",
    "\n",
    "answer = chain.invoke('세종대왕의 즉위기간과 어떤 업적을 남겼나요?')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(model = 'gemma2-9b-it', temperature=0) \n",
    "\n",
    "response = model.invoke('LLM의 역사에 대해서 설명해줘')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='## LLM의 역사: 인공지능의 언어 능력 발전\\n\\nLLM(Large Language Model, 대규모 언어 모델)은 방대한 양의 텍스트 데이터를 학습하여 인간과 유사한 수준의 언어를 이해하고 생성할 수 있는 인공지능 모델입니다. \\n\\nLLM의 역사는 몇 가지 중요한 단계로 나눌 수 있습니다.\\n\\n**1. 초기 단계 (1950년대 - 1980년대):**\\n\\n* **1950년대:** 튜링 테스트 등 인공지능의 기초가 다지어지고, 컴퓨터가 언어를 이해하고 생성할 수 있는 가능성이 제기됩니다.\\n* **1960년대 - 1970년대:** ELIZA와 같은 시스템이 개발되어 텍스트 기반 대화를 시도했지만, 제한적인 문맥 이해와 생성 능력을 보였습니다.\\n* **1980년대:** 규칙 기반 시스템과 확률적 모델이 개발되어 자연어 처리 기술이 발전했습니다.\\n\\n**2. 딥러닝 시대 (1990년대 - 2010년대):**\\n\\n* **1990년대:** 신경망 기반 모델이 등장하며 자연어 처리 분야에 딥러닝이 적용되기 시작했습니다.\\n* **2000년대:** RNN(Recurrent Neural Network)이 등장하여 순차적인 데이터 처리에 효과적이라는 사실이 밝혀지면서 자연어 처리 성능이 향상되었습니다.\\n* **2010년대:** LSTM(Long Short-Term Memory)과 GRU(Gated Recurrent Unit)와 같은 RNN 변형 모델이 개발되어 더욱 복잡한 문맥을 이해하는 능력을 보여주었습니다.\\n\\n**3. 대규모 언어 모델의 등장 (2017년 - 현재):**\\n\\n* **2017년:** Google의 Transformer 모델이 등장하여 자연어 처리 분야에 혁명을 일으켰습니다. Transformer는 RNN보다 더 효율적으로 긴 문장을 처리할 수 있으며, Attention 메커니즘을 통해 문맥 정보를 더 잘 이해합니다.\\n* **2018년:** OpenAI의 GPT(Generative Pre-trained Transformer) 모델이 발표되어 대규모 언어 모델의 시대를 열었습니다. GPT는 방대한 텍스트 데이터를 학습하여 다양한 언어 작업을 수행할 수 있는 능력을 보여주었습니다.\\n* **2020년대:** GPT-3, BERT, LaMDA 등 더욱 큰 규모와 성능을 가진 LLM들이 계속해서 개발되고 있습니다. 이러한 모델들은 다양한 분야에서 활용되고 있으며, 인공지능 기술의 발전에 큰 기여를 하고 있습니다.\\n\\n**LLM의 미래:**\\n\\nLLM은 앞으로도 계속해서 발전할 것으로 예상됩니다. 더 큰 규모, 더 높은 성능, 더 다양한 기능을 가진 LLM들이 개발되면서 인간의 언어 이해와 생성 능력을 뛰어넘는 시대가 다가올 수 있습니다.\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 751, 'prompt_tokens': 20, 'total_tokens': 771, 'completion_time': 1.365454545, 'prompt_time': 0.001902586, 'queue_time': 0.018878771, 'total_time': 1.367357131}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-92cf6da5-5c4b-4461-a832-8cf522661f0a-0', usage_metadata={'input_tokens': 20, 'output_tokens': 751, 'total_tokens': 771})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "771"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata['token_usage']['total_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0017734852542153046"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.response_metadata['token_usage']['total_time'] / response.response_metadata['token_usage']['total_tokens']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
