{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **FewShotPromptTemplate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Few-shot 학습은 언어 모델에 몇 가지 예시를 제공하여 특정 작업을 수행하도록 유도하는 기법.\n",
    "\n",
    "- Few-shot 학습을 활용함으로써, 언어 모델은 주어진 예제들을 참고하여 더 정확하고 일관된 응답을 생성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Few-shot 예제 포맷터 생성**\n",
    "\n",
    "- Few-shot 예제를 포맷팅하기 위한 템플릿을 생성. \n",
    "\n",
    "- PromptTemplate 은 질문과 답변을 포함하는 구조 \n",
    "\n",
    "- 템플릿은 각 예제를 일관된 형식으로 표현할 수 있게 해주어, 모델이 입력과 출력의 패턴을 쉽게 인식."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['answer', 'question'], input_types={}, partial_variables={}, template='질문: {question}\\n답변 : {answer}')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "example_prompt = PromptTemplate.from_template(\"질문: {question}\\n답변 : {answer}\")\n",
    "\n",
    "example_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. 예제**\n",
    "\n",
    "- 모델이 참조 할 수 있는 Q&A 예제 세트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
    "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
    "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
    "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
    "최종 답변은: 아인슈타인\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"네이버의 창립자는 언제 태어났나요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 네이버의 창립자는 누구인가요?\n",
    "중간 답변: 네이버는 이해진에 의해 창립되었습니다.\n",
    "추가 질문: 이해진은 언제 태어났나요?\n",
    "중간 답변: 이해진은 1967년 6월 22일에 태어났습니다.\n",
    "최종 답변은: 1967년 6월 22일\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"율곡 이이의 어머니가 태어난 해의 통치하던 왕은 누구인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 율곡 이이의 어머니는 누구인가요?\n",
    "중간 답변: 율곡 이이의 어머니는 신사임당입니다.\n",
    "추가 질문: 신사임당은 언제 태어났나요?\n",
    "중간 답변: 신사임당은 1504년에 태어났습니다.\n",
    "추가 질문: 1504년에 조선을 통치한 왕은 누구인가요?\n",
    "중간 답변: 1504년에 조선을 통치한 왕은 연산군입니다.\n",
    "최종 답변은: 연산군\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"올드보이와 기생충의 감독이 같은 나라 출신인가요?\",\n",
    "        \"answer\": \"\"\"이 질문에 추가 질문이 필요한가요: 예.\n",
    "추가 질문: 올드보이의 감독은 누구인가요?\n",
    "중간 답변: 올드보이의 감독은 박찬욱입니다.\n",
    "추가 질문: 박찬욱은 어느 나라 출신인가요?\n",
    "중간 답변: 박찬욱은 대한민국 출신입니다.\n",
    "추가 질문: 기생충의 감독은 누구인가요?\n",
    "중간 답변: 기생충의 감독은 봉준호입니다.\n",
    "추가 질문: 봉준호는 어느 나라 출신인가요?\n",
    "중간 답변: 봉준호는 대한민국 출신입니다.\n",
    "최종 답변은: 예\n",
    "\"\"\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문: 스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\n",
      "답변 : 이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
      "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
      "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
      "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
      "최종 답변은: 아인슈타인\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  ** 연산자는 딕셔너리의 key-value 형태를 풀어서 함수나 메서드에 개별 인자로 전달하는 역할\n",
    "\n",
    "print(example_prompt.format(**examples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. FewShotPromptTemplate 생성**\n",
    "\n",
    "\n",
    "- Few-shot 프롬프트 템플릿을 생성, 새로운 질문에 대한 프롬프트를 생성. \n",
    "\n",
    "- FewShotPromptTemplate 은 예제들을 결합하고 새로운 입력을 추가하여 최종 프롬프트를 생성."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "답변 : 이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: 구글의 창립자는 누구인가요?\n",
      "중간 답변: 구글의 창립자는 래리 페이지와 세르게이 브린입니다.\n",
      "추가 질문: 래리 페이지는 언제 태어났나요?\n",
      "중간 답변: 래리 페이지는 1973년 3월 26일에 태어났습니다.\n",
      "추가 질문: 세르게이 브린은 언제 태어났나요?\n",
      "중간 답변: 세르게이 브린은 1973년 8월 21일에 태어났습니다.\n",
      "최종 답변은: 래리 페이지는 1973년 3월 26일, 세르게이 브린은 1973년 8월 21일에 태어났습니다. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# FewShotPromptTemplate\n",
    "prompt = FewShotPromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    "    suffix = \"질문: {input}\",\n",
    "    input_variables = [\"input\"],\n",
    ")\n",
    "# 새로운 질문에 대한 프롬프트를 생성하고 출력합니다.\n",
    "# print(prompt.invoke({\"input\": \"화성의 표면이 붉은 이유는 무엇인가요?\"}).to_string())\n",
    "\n",
    "final_prompt = prompt.format(input = \"구글의 창립자는 언제 태어났나요?\")\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "answer = llm.invoke(final_prompt)\n",
    "\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. 예제 선택기 사용하기**\n",
    "\n",
    "\n",
    "- 의미적 유사성을 기반으로 가장 관련성 높은 예제를 선택. \n",
    "\n",
    "- `SemanticSimilarityExampleSelector` 는 입력 질문과 가장 유사한 예제를 선택."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력과 가장 유사한 예제: 헬렌켈러와 히틀러중 누가 더 오래 살았나요?\n",
      "\n",
      "\n",
      "answer: 이 질문에 추가 질문이 필요한가요: 예.\n",
      "추가 질문: 스티브 잡스는 몇 살에 사망했나요?\n",
      "중간 답변: 스티브 잡스는 56세에 사망했습니다.\n",
      "추가 질문: 아인슈타인은 몇 살에 사망했나요?\n",
      "중간 답변: 아인슈타인은 76세에 사망했습니다.\n",
      "최종 답변은: 아인슈타인\n",
      "\n",
      "question: 스티브 잡스와 아인슈타인 중 누가 더 오래 살았나요?\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# SemanticSimilarityExampleSelector를 초기화합니다.\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,            # 사용할 예제들\n",
    "    OpenAIEmbeddings(),  # 임베딩 모델\n",
    "    Chroma,              # 벡터 저장소\n",
    "    k=1,                 # 선택할 예제 수\n",
    ")\n",
    "\n",
    "# 새로운 질문에 대해 가장 유사한 예제를 선택합니다.\n",
    "question = \"헬렌켈러와 히틀러중 누가 더 오래 살았나요?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "\n",
    "print(f\"입력과 가장 유사한 예제: {question}\")\n",
    "for example in selected_examples:\n",
    "    print(\"\\n\")\n",
    "    for k, v in example.items():\n",
    "        print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **채팅 모델에서 Few-shot 예제 사용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. 고정 예제 사용하기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM은 **Large Language Model**의 약자입니다. \n",
      "\n",
      "즉, 대규모 언어 모델이라는 뜻입니다. 😊 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import FewShotChatMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "# 예제 정의 \n",
    "# 예제 정의\n",
    "examples = [\n",
    "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
    "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
    "]\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        \n",
    "        ('human', \"{input}\"),\n",
    "        ('ai', \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Few-shot 프롬프트 템플릿 정의\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    examples = examples,\n",
    "    example_prompt = example_prompt,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 템플릿 생성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델과 체인 생성\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0)\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 모델에 질문하기\n",
    "result = chain.invoke({\"input\": \"LLM은 무엇의 약자인가요?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "태양계에서 가장 큰 행성은 목성입니다. 🪐  \n",
      "\n",
      "\n",
      "목성은 지구의 약 11배나 큰 규모를 자랑합니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 더 많은 예제 추가\n",
    "examples = [\n",
    "    {\"input\": \"지구의 대기 중 가장 많은 비율을 차지하는 기체는 무엇인가요?\", \"output\": \"질소입니다.\"},\n",
    "    {\"input\": \"광합성에 필요한 주요 요소들은 무엇인가요?\", \"output\": \"빛, 이산화탄소, 물입니다.\"},\n",
    "    {\"input\": \"피타고라스 정리를 설명해주세요.\", \"output\": \"직각삼각형에서 빗변의 제곱은 다른 두 변의 제곱의 합과 같습니다.\"},\n",
    "    {\"input\": \"DNA의 기본 구조를 간단히 설명해주세요.\", \"output\": \"DNA는 이중 나선 구조를 가진 핵산입니다.\"},\n",
    "    {\"input\": \"원주율(π)의 정의는 무엇인가요?\", \"output\": \"원의 둘레와 지름의 비율입니다.\"},\n",
    "]\n",
    "\n",
    "# 벡터 저장소 생성\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma.from_texts(to_vectorize, embeddings, metadatas=examples)\n",
    "\n",
    "# 예제 선택기 생성\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=vectorstore,\n",
    "    k=2,\n",
    ")\n",
    "\n",
    "# Few-shot 프롬프트 템플릿 생성\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 템플릿 생성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 과학과 수학에 대해 잘 아는 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "# 모델과 체인 생성\n",
    "model = ChatGroq(model=\"gemma2-9b-it\", temperature=0)\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 모델에 질문하기\n",
    "result = chain.invoke('태양계에서 가장 큰 행성은 무엇인가요?')\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FewShotChatMessagePromptTemplate(example_selector=SemanticSimilarityExampleSelector(vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001BA5D4E8D50>, k=2, example_keys=None, input_keys=None, vectorstore_kwargs=None), input_variables=[], input_types={}, partial_variables={}, example_prompt=ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "few_shot_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
