{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ê¸°ë³¸ ì²´ì¸**\n",
    "\n",
    "#### **ChatPromptTemplate + Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# prompt + model + output parser\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  ('system', ' ë‹¹ì‹ ì€ ì‚¬ìš©ìì— ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” AI ëª¨ë¸ì´ê³  ì´ë¦„ì€ Gemma, ê·¸ë¦¬ê³  ë‹¹ì‹ ì€ êµ¬ê¸€ì—ì„œ ë°œí‘œëœ Open Soruce LLM ëª¨ë¸ì…ë‹ˆë‹¤'),\n",
    "   ('user', '{input}'),\n",
    "  \n",
    "])\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.stream({'input' : 'ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜'})\n",
    "\n",
    "print(response.content)\n",
    "print(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ChatPromptTemplate + Model + StrOutputParser(ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ)**\n",
    "\n",
    "StrOutputParser(ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ) : ë¬¸ìì—´íŒŒì‹±ì„ í†µí•´ content ì—†ì´ ë¬¸ìì—´ì„ ì¶œë ¥í•´ì¤Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Gemmaì…ë‹ˆë‹¤. êµ¬ê¸€ì—ì„œ ê°œë°œëœ ì˜¤í”ˆ ì†ŒìŠ¤ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì…ë‹ˆë‹¤. \\n\\nì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ ì¢€ ë” ìì„¸íˆ ì•Œë ¤ë“œë¦´ê¹Œìš”? í¥ë¯¸ë¡œìš´ ì£¼ì œì¸ ë§Œí¼ ì—¬ëŸ¬ ì¸¡ë©´ì—ì„œ ì„¤ëª…í•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì–´ë–¤ ë¶€ë¶„ì— ê°€ì¥ ê´€ì‹¬ì´ ìˆìœ¼ì‹ ê°€ìš”?\\n\\nì˜ˆë¥¼ ë“¤ì–´, ì¸ê³µì§€ëŠ¥ì´ \\n\\n* **ë¬´ì—‡ì¸ê°€?**  (ë§ˆì¹˜ ìš°ë¦¬ ë‡Œì²˜ëŸ¼ í•™ìŠµí•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì»´í“¨í„° í”„ë¡œê·¸ë¨)\\n* **ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ê°€?** (ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  íŒ¨í„´ì„ ë°°ìš°ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©)\\n* **ì–´ë–¤ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë˜ëŠ”ê°€?** (ìë™ì°¨, ì˜ë£Œ, ê¸ˆìœµ, êµìœ¡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼)\\n\\n* **ë¯¸ë˜ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹  ê²ƒì¸ê°€?** (ìƒí™œì˜ ë³€í™”, ìƒˆë¡œìš´ ê¸°íšŒ, ìœ¤ë¦¬ì  ë¬¸ì œ ë“±)\\n\\nì— ëŒ€í•´ ì•Œê³  ì‹¶ìœ¼ì‹ ê°€ìš”? \\n\\ní˜¹ì‹œ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ìˆìœ¼ì‹œë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”. ì œê°€ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ğŸ˜Š \\n\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# prompt + model + output parser\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  ('system', ' ë‹¹ì‹ ì€ ì‚¬ìš©ìì— ì§ˆë¬¸ì— ëŒ€í•´ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” AI ëª¨ë¸ì´ê³  ì´ë¦„ì€ Gemma, ê·¸ë¦¬ê³  ë‹¹ì‹ ì€ êµ¬ê¸€ì—ì„œ ë°œí‘œëœ Open Soruce LLM ëª¨ë¸ì…ë‹ˆë‹¤'),\n",
    "   ('user', '{input}'),\n",
    "  \n",
    "])\n",
    "\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "answer = chain.invoke({'input' : 'ì¸ê³µì§€ëŠ¥ì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜'})\n",
    "\n",
    "answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
