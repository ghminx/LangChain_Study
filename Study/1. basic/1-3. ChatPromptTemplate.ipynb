{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **ChatPromptTemplate**\n",
    "\n",
    "- ChatPromptTemplate은 대화형 상황에서 여러 메시지 입력을 기반으로 단일 메시지 응답을 생성하는 데 사용. 이는 대화형 모델이나 챗봇 개발에 주로 사용. 입력은 여러 메시지를 원소로 갖는 리스트로 구성되며, 각 메시지는 역할(role)과 내용(content)으로 구성\n",
    "\n",
    "\n",
    "    - SystemMessage: 시스템의 기능을 설명합니다.\n",
    "    - HumanMessage: 사용자의 질문을 나타냅니다.\n",
    "    - AIMessage: AI 모델의 응답을 제공합니다.\n",
    "    - FunctionMessage: 특정 함수 호출의 결과를 나타냅니다.\n",
    "    - ToolMessage: 도구 호출의 결과를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **from_template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: tell me about dog\n",
      "tell me about dog\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template('tell me about {input}')\n",
    "\n",
    "print(prompt.format(input = 'dog'))  # format_messages 사용가능\n",
    "\n",
    "\n",
    "prompt1 = PromptTemplate.from_template('tell me about {input}')\n",
    "\n",
    "print(prompt1.format(input = 'dog'))\n",
    "\n",
    "\n",
    "# ChatPromptTemplate.from_template\n",
    "# 설명: 이 메서드는 입력된 템플릿(문자열)을 내부적으로 HumanMessage 객체로 변환합니다.\n",
    "# 의미: 결과적으로 대화 형식의 메시지 리스트가 만들어지고, 모델이 이를 \"사람(Human)이 말한 메시지\"로 명확히 인식할 수 있도록 구조화됩니다.\n",
    "# 핵심: 대화 기반 모델(Chat 모델)이 이해하기 쉽게 역할(human)이 태깅된 형태로 전달됩니다.\n",
    "\n",
    "# PromptTemplate.from_template\n",
    "# 설명: 이 메서드는 템플릿을 일반 텍스트(문자열)로만 처리합니다.\n",
    "# 의미: 어떤 역할이나 구조 없이 순수한 텍스트로 남아 있어서, 모델이 이를 특정 역할로 인식하지 않고 그냥 텍스트 입력으로 봅니다.\n",
    "# 핵심: 주로 텍스트 완성 모델이나 역할 구분이 필요 없는 작업에 적합합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **from_message**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='자연어처리의 전문가로서 LLM에 대해서 잘 설명해줄수있는 AI', additional_kwargs={}, response_metadata={}), HumanMessage(content='자연어처리 4주 공부 코스 만들어줘', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'##  AI 자연어 처리 4주 공부 코스: LLM을 이해하기\\n\\n**이 코스는 LLM (Large Language Model) 에 대한 이해를 넓히고, 자연어 처리 (NLP) 분야의 기본 지식을 쌓는 것을 목표로 합니다.** \\n\\n**코스 구성:**\\n\\n**주차 1: 자연어 처리의 기초**\\n\\n* **NLP 개념 및 역사:** 자연어 처리란 무엇이며, 왜 중요한가? NLP의 역사와 주요 전환점\\n* **텍스트 전처리:** 텍스트 데이터를 NLP 알고리즘에 적합하게 변환하는 방법\\n    * 토큰화, 문장 분리, 불필요한 문자 제거, 정규화 등\\n* **단어 임베딩:** 단어를 벡터로 표현하는 방법 이해 및 활용\\n    * Word2Vec, GloVe, FastText 등\\n* **TF-IDF:** 문서에서 단어의 중요도를 측정하는 기법\\n\\n**주차 2: 기본적인 자연어 처리 알고리즘**\\n\\n* **텍스트 분류:** 텍스트를 주어진 범주로 분류하는 방법\\n    * Naive Bayes, Logistic Regression, Support Vector Machine 등\\n* **문서 요약:** 긴 텍스트를 짧은 요약문으로 변환하는 방법\\n    * Extractive Summarization, Abstractive Summarization\\n* **감정 분석:** 텍스트에서 감정을 인식하는 방법\\n    * Lexicon-based Approach, Machine Learning Approach\\n* **Named Entity Recognition (NER):** 텍스트에서 사람, 장소, 기관 등을 식별하는 방법\\n\\n**주차 3: LLM 도입 및 이해**\\n\\n* **LLM 개념 및 역사:** Transformer, BERT, GPT 등 주요 LLM 모델 이해\\n* **LLM 학습 방법:** 훈련 데이터, Fine-tuning, Prompt Engineering\\n* **LLM 응용 사례:**\\n    * 챗봇, 번역, 질문 답변, 텍스트 생성 등\\n* **LLM의 한계점:** 편향, 오류, 안전성 문제\\n\\n**주차 4: 심화 학습 및 LLM 활용**\\n\\n* **최신 LLM 기술 트렌드:** OpenAI API, HuggingFace Transformers 등\\n* **LLM을 활용한 프로젝트:** \\n    * 챗봇 개발, 텍스트 생성, 요약, 번역 등\\n* **LLM 개발 윤리:** 편향, 오류, 안전성 등 윤리적 문제 고려\\n* **자연어 처리 분야의 미래 전망:** LLM의 발전과 미래 전망 \\n\\n**학습 자료:**\\n\\n* **온라인 강좌:** Coursera, edX, Udacity 등\\n* **책:**\\n    * \"Speech and Language Processing\" by Jurafsky & Martin\\n    * \"Deep Learning with Python\" by Francois Chollet\\n    * \"Natural Language Processing with Transformers\" by HuggingFace\\n* **웹사이트:**\\n    * HuggingFace: https://huggingface.co/\\n    * OpenAI: https://openai.com/\\n    * TensorFlow: https://www.tensorflow.org/\\n    * PyTorch: https://pytorch.org/\\n\\n\\n**주의:**\\n\\n* 이 코스는 LLM에 대한 기본적인 이해를 돕기 위한 가이드라인이며, \\n* 학습 속도는 개인의 기초 지식 및 학습 능력에 따라 달라질 수 있습니다.\\n* 꾸준한 학습과 실습을 통해 LLM을 깊이 이해하고 다양한 분야에 적용할 수 있습니다.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"자연어처리의 전문가로서 LLM에 대해서 잘 설명해줄수있는 AI\"),\n",
    "     ('user', \"{input}\"),\n",
    "])\n",
    "\n",
    "\n",
    "messages = chat_prompt.format_messages(input = '자연어처리 4주 공부 코스 만들어줘')\n",
    "\n",
    "print(messages)\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## LLM에 대한 설명 및 PyTorch로 이진 분류 모델 구현\n",
      "\n",
      "### 1. LLM (Large Language Model)에 대한 설명\n",
      "\n",
      "LLM은 방대한 텍스트 데이터를 학습하여 다양한 자연어 처리 작업을 수행할 수 있는 강력한 딥러닝 모델입니다. \n",
      "\n",
      "**핵심 특징:**\n",
      "\n",
      "* **방대한 파라미터:** 수십억 또는 수조 개의 파라미터를 가지고 있어 복잡한 언어 패턴을 학습할 수 있습니다.\n",
      "* **Transformer 구조:** 주로 Transformer 아키텍처를 기반으로 하여 문맥을 효과적으로 이해하고 처리할 수 있습니다.\n",
      "* **다양한 작업 수행:** 텍스트 생성, 번역, 요약, 질문 답변, 분류 등 다양한 자연어 처리 작업에 활용됩니다.\n",
      "\n",
      "**예시:** GPT-3, BERT, LaMDA\n",
      "\n",
      "### 2. PyTorch로 이진 분류 모델 구현\n",
      "\n",
      "이진 분류는 주어진 데이터를 두 개의 범주로 분류하는 작업입니다. PyTorch를 사용하여 이진 분류 모델을 구현하는 방법은 다음과 같습니다.\n",
      "\n",
      "**1. 데이터 준비:**\n",
      "\n",
      "* 데이터를 레이블(0 또는 1)이 있는 샘플로 구성합니다.\n",
      "* 데이터를 학습 set, 검증 set, 테스트 set으로 분할합니다.\n",
      "\n",
      "**2. 모델 정의:**\n",
      "\n",
      "* PyTorch의 `torch.nn` 모듈을 사용하여 모델을 정의합니다. 간단한 예시로는 다음과 같은 모델을 사용할 수 있습니다.\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class BinaryClassifier(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, output_size):\n",
      "        super(BinaryClassifier, self).__init__()\n",
      "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
      "        self.relu = nn.ReLU()\n",
      "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
      "        self.sigmoid = nn.Sigmoid()\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = self.fc1(x)\n",
      "        x = self.relu(x)\n",
      "        x = self.fc2(x)\n",
      "        x = self.sigmoid(x)\n",
      "        return x\n",
      "```\n",
      "\n",
      "* `input_size`: 입력 데이터의 크기\n",
      "\n",
      "* `hidden_size`: 은닉 층의 노드 수\n",
      "\n",
      "* `output_size`: 출력 레이어의 노드 수 (이진 분류이므로 1)\n",
      "\n",
      "**3. 손실 함수 및 최적화 알고리즘:**\n",
      "\n",
      "* 이진 분류에 적합한 손실 함수로는 Binary Cross Entropy Loss를 사용합니다.\n",
      "* 최적화 알고리즘으로는 Adam, SGD 등을 사용할 수 있습니다.\n",
      "\n",
      "```python\n",
      "criterion = nn.BCELoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
      "```\n",
      "\n",
      "**4. 모델 훈련:**\n",
      "\n",
      "* 학습 데이터를 모델에 입력하고, 출력값을 실제 레이블과 비교하여 손실 함수를 계산합니다.\n",
      "* 손실 함수를 기반으로 모델의 파라미터를 업데이트합니다.\n",
      "* 여러 에포크 동안 반복하여 모델을 훈련합니다.\n",
      "\n",
      "```python\n",
      "for epoch in range(num_epochs):\n",
      "    for batch_idx, (data, target) in enumerate(train_loader):\n",
      "        optimizer.zero_grad()\n",
      "        output = model(data)\n",
      "        loss = criterion(output, target.float())\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        # ... (정규화, 검증 세트 성능 확인 등)\n",
      "```\n",
      "\n",
      "**5. 모델 평가:**\n",
      "\n",
      "* 훈련된 모델을 테스트 세트에 적용하여 성능을 평가합니다.\n",
      "\n",
      "**참고:**\n",
      "\n",
      "* 위 코드는 간단한 예시이며, 실제로는 데이터 전처리, 하이퍼파라미터 튜닝, 모델 구조 변경 등 다양한 작업이 필요할 수 있습니다.\n",
      "* PyTorch 문서와 다양한 온라인 자료를 참고하여 자세한 내용을 학습할 수 있습니다.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chain을 사용할땐 입력값은 딕셔너리 format 사용하지 않음 \n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"자연어처리의 전문가로서 LLM에 대해서 잘 설명해줄수있는 AI\"),\n",
    "     ('user', \"{input}\"),\n",
    "])\n",
    "\n",
    "\n",
    "llm = ChatGroq(model = 'gemma2-9b-it')\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = chat_prompt | llm | parser\n",
    "\n",
    "\n",
    "response = chain.invoke({'input' : 'pytorch로 이진 분류 모델 만드는 방법과 코드를 제공해줘'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='인공지능 모델 학습은 사람이 공부하는 것과 비슷하지만, 좀 더 컴퓨터적인 방식이에요. \\n\\n**1. 데이터 탐험:** \\n\\n사람이 책이나 경험을 통해 지식을 습득하듯, AI 모델은 **데이터를 통해 학습**합니다. 이 데이터는 텍스트, 이미지, 소리, 숫자 등 여러 형태로 존재할 수 있으며, 모델이 이해하고 분석할 수 있도록 **정리**하는 과정이 필요해요. 예를 들어, 이미지 인식 모델은 수많은 이미지와 그 이미지에 대한 설명(레이블)을 학습합니다.\\n\\n**2. 패턴 찾기:**\\n\\n모델은 데이터 속에 숨겨진 **패턴**을 찾아내기 위해 끊임없이 노력합니다. \\n\\n예를 들어, 이미지 인식 모델은 고양이 사진들을 보면서 \"털이 멀고, 귀가 뾰족한\", \"꼬리가 길고 털갈이를 하는\" 등의 패턴을 학습합니다. \\n\\n**3. 공식 만들기:**\\n\\n모델은 찾아낸 패턴을 바탕으로 **예측 규칙 (모델)**을 만들어요. \\n\\n예를 들어, 고양이 이미지 인식 모델은 \"털이 멀고, 귀가 뾰족한, 꼬리가 길고 털갈이를 하는\" 특징을 가진 이미지는 고양이라고 예측할 수 있는 규칙을 만들어요.\\n\\n**4. 점검 및 수정:**\\n\\n만들어진 규칙의 정확도를 **테스트**하고, 틀린 예측을 분석하여 **규칙을 수정**하는 과정을 반복합니다. \\n\\n이 과정을 통해 모델은 점점 더 정확하고 완벽한 예측 능력을 갖추게 됩니다.\\n\\n**간단히 말해, AI 모델은 데이터를 통해 패턴을 배우고, 그 패턴을 바탕으로 예측 규칙을 만들고, 꾸준히 점검하며 수정하는 과정을 통해 학습합니다.**\\n\\n\\n', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 483, 'prompt_tokens': 29, 'total_tokens': 512, 'completion_time': 0.878181818, 'prompt_time': 0.002153356, 'queue_time': 0.017849753000000003, 'total_time': 0.880335174}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-637386eb-d98b-42aa-a3a0-47c49e941273-0', usage_metadata={'input_tokens': 29, 'output_tokens': 483, 'total_tokens': 512})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt 를 PromptTemplate 객체로 생성합니다.\n",
    "prompt = PromptTemplate.from_template(\"{topic} 에 대해 쉽게 설명해주세요.\")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "input = {'topic' : '인공지능 모델의 학습 원리'}\n",
    "\n",
    "chain.invoke(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
